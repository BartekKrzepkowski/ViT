{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d39ef611-b2b8-4a9b-9020-dcda4c2f00ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a1b429-0e3d-4ea6-a211-2e6dd60a1edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ed85b9e-ef8a-4931-b3dc-7a3b1eb8f274",
   "metadata": {},
   "source": [
    "# TODO\n",
    "\n",
    "> Zbudować model\n",
    "> podzielić zdjęcia na patche 4x4, albo 7x7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc227cfa-1e6d-487e-8a9d-e4da0d61116a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from einops import rearrange\n",
    "\n",
    "class ViT(nn.Module):\n",
    "    def __init__(self,\n",
    "                 img_dim,\n",
    "                 in_channels=3,\n",
    "                 patch_dim=16,\n",
    "                 num_classes=10,\n",
    "                 dim=512,\n",
    "                 blocks=6,\n",
    "                 heads=4,\n",
    "                 dim_linear_block=1024,\n",
    "                 dim_head=None,\n",
    "                 dropout=0, transformer=None, classification=True):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            img_dim: the spatial image size\n",
    "            in_channels: number of img channels\n",
    "            patch_dim: desired patch dim\n",
    "            num_classes: classification task classes\n",
    "            dim: the linear layer's dim to project the patches for MHSA\n",
    "            blocks: number of transformer blocks\n",
    "            heads: number of heads\n",
    "            dim_linear_block: inner dim of the transformer linear block\n",
    "            dim_head: dim head in case you want to define it. defaults to dim/heads\n",
    "            dropout: for pos emb and transformer\n",
    "            transformer: in case you want to provide another transformer implementation\n",
    "            classification: creates an extra CLS token\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        assert img_dim % patch_dim == 0, f'patch size {patch_dim} not divisible'\n",
    "        self.p = patch_dim\n",
    "        self.classification = classification\n",
    "        tokens = (img_dim // patch_dim) ** 2\n",
    "        self.token_dim = in_channels * (patch_dim ** 2)\n",
    "        self.dim = dim\n",
    "        self.dim_head = (int(dim / heads)) if dim_head is None else dim_head\n",
    "        self.project_patches = nn.Linear(self.token_dim, dim)\n",
    "\n",
    "        self.emb_dropout = nn.Dropout(dropout)\n",
    "        if self.classification:\n",
    "            self.cls_token = nn.Parameter(torch.randn(1, 1, dim))\n",
    "            self.pos_emb1D = nn.Parameter(torch.randn(tokens + 1, dim))\n",
    "            self.mlp_head = nn.Linear(dim, num_classes)\n",
    "        else:\n",
    "            self.pos_emb1D = nn.Parameter(torch.randn(tokens, dim))\n",
    "\n",
    "        if transformer is None:\n",
    "            self.mhsa_layer = nn.TransformerEncoderLayer(dim, self.dim_head, dim_feedforward=dim_linear_block, batch_first=True, activation=nn.GELU())\n",
    "            self.mhsa = nn.TransformerEncoder(self.mhsa_layer, num_layers=blocks, norm=nn.LayerNorm(dim))\n",
    "        else:\n",
    "            self.transformer = transformer\n",
    "\n",
    "    def expand_cls_to_batch(self, batch):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            batch: batch size\n",
    "        Returns: cls token expanded to the batch size\n",
    "        \"\"\"\n",
    "        return self.cls_token.expand([batch, -1, -1])\n",
    "\n",
    "    def forward(self, img, mask=None):\n",
    "        batch_size = img.shape[0]\n",
    "        img_patches = rearrange(\n",
    "            img, 'b c (patch_x x) (patch_y y) -> b (x y) (patch_x patch_y c)',\n",
    "                                patch_x=self.p, patch_y=self.p)\n",
    "        # project patches with linear layer + add pos emb\n",
    "        img_patches = self.project_patches(img_patches)\n",
    "\n",
    "        if self.classification:\n",
    "            img_patches = torch.cat(\n",
    "                (self.expand_cls_to_batch(batch_size), img_patches), dim=1)\n",
    "\n",
    "        patch_embeddings = self.emb_dropout(img_patches + self.pos_emb1D)\n",
    "\n",
    "        # feed patch_embeddings and output of transformer. shape: [batch, tokens, dim]\n",
    "        y = self.mhsa(patch_embeddings, mask)\n",
    "\n",
    "        if self.classification:\n",
    "            # we index only the cls token for classification. nlp tricks :P\n",
    "            return self.mlp_head(y[:, 0, :])\n",
    "        else:\n",
    "            return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ed1e53-475f-416b-ae3a-8505d535510f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import MNIST, CIFAR10\n",
    "from torchvision.transforms import Compose, ToTensor, Normalize\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "transform = Compose([ToTensor(), Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "dataset_train = CIFAR10('./data', train=True, transform=transform, download=True)\n",
    "dataset_test = CIFAR10('./data', train=False, transform=transform, download=True)\n",
    "\n",
    "loader_train = DataLoader(dataset_train, batch_size=64, shuffle=True, pin_memory=True, num_workers=4)\n",
    "loader_test = DataLoader(dataset_test, batch_size=64, shuffle=False, pin_memory=True, num_workers=4)\n",
    "\n",
    "loaders = {\n",
    "    'train': loader_train,\n",
    "    'test': loader_test\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1889914-1710-46bc-a744-a2941e9521d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_patches_naive(x, step):\n",
    "    patches = []\n",
    "    for i in range(x.shape[-1]//step):\n",
    "        for j in range(x.shape[-1]//step):\n",
    "            patches.append(x[:,:,i*step:(i+1)*step,j*step:(j+1)*step].flatten(start_dim=1))\n",
    "    return torch.stack(patches, axis=0).transpose(0,1)\n",
    "\n",
    "# patches = get_patches_naive(x, 4)\n",
    "# patches.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29680358-01a9-422e-99b3-13a1e426aaf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from tqdm.auto import tqdm\n",
    "from tensorboard_pytorch import TensorboardPyTorch\n",
    "\n",
    "def simple_trainer(model, loaders, criterion, optim, writer, epoch_start, epoch_end, phases=['train', 'test']):\n",
    "    for epoch in tqdm(range(epoch_start, epoch_end)):\n",
    "        for phase in phases:\n",
    "            running_acc = 0.0\n",
    "            running_loss = 0.0\n",
    "            model.train() if 'train' in phase else model.eval()\n",
    "            for x_true, y_true in loaders[phase]:\n",
    "                x_true, y_true = x_true.to(device), y_true.to(device)\n",
    "                # x_true = get_patches_naive(x_true, 16)\n",
    "                y_pred = model(x_true)\n",
    "                loss = criterion(y_pred, y_true)\n",
    "                if phase == phases[0]:\n",
    "                    optim.zero_grad()\n",
    "                    loss.backward()\n",
    "                    optim.step()\n",
    "                running_acc += (torch.argmax(y_pred.detach().data, dim=1) == y_true).sum().item()\n",
    "                running_loss += loss.item() * x_true.size(0)\n",
    "\n",
    "            epoch_acc = running_acc / len(loaders[phase].dataset)\n",
    "            epoch_loss = running_loss / len(loaders[phase].dataset)\n",
    "            writer.log_scalar(f'Acc/{phase}', round(epoch_acc, 4), epoch + 1)\n",
    "            writer.log_scalar(f'Loss/{phase}', round(epoch_loss, 4), epoch + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b84a6c3f-6970-4142-bb40-dec20d2a4630",
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir=tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba659a5-7a95-4a38-bf69-2db42b25ea16",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 100\n",
    "import madgrad\n",
    "from ImageTransformer import ViT\n",
    "\n",
    "model = ViT(\n",
    "        patch_height = 16,\n",
    "        patch_width = 16,\n",
    "        embedding_dims = 768,\n",
    "        dropout = 0.1,\n",
    "        heads = 4,\n",
    "        num_layers = 4,\n",
    "        forward_expansion = 4,\n",
    "        max_len = int((32*32)/(16*16)),\n",
    "        layer_norm_eps = 1e-5,\n",
    "        num_classes = 10,\n",
    "    ).to(device)\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "# optim = madgrad.MADGRAD(model.parameters(), lr=1e-2, momentum=0.9)\n",
    "optim = torch.optim.Adam(model.parameters(), lr=1e-2)\n",
    "date = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "writer = TensorboardPyTorch(f'tensorboard/ViT/cifar10/sgd/ShivamRajSharma_model/{date}', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6edd2f53-a5f1-4fa1-b564-aa995006d47f",
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_trainer(model, loaders, criterion, optim, writer, epoch_start=0, epoch_end=EPOCHS, phases=['train', 'test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff31c9b-bbf6-41b4-bb9e-5711b647530f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a15380b5-976d-408f-b2b8-db117e92d5c3",
   "metadata": {},
   "source": [
    "# Setting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "140092d4-fe34-4b79-a45a-241dded72fa1",
   "metadata": {},
   "source": [
    "# Learned Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dbd1ab3-8b9f-4819-8e70-51caee5ae27e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4839188-8a7e-4a37-8f12-e79b65c717a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(model.pos_emb.detach().cpu().squeeze(0))\n",
    "plt.xlabel('Emb Dim')\n",
    "plt.ylabel('Position')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b5ce9fc-3fea-4dd2-a538-3227bc201421",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.pos_emb.detach().cpu().squeeze(0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ccef54-2458-404c-a5a8-9088a7286883",
   "metadata": {},
   "outputs": [],
   "source": [
    "mult = 8\n",
    "\n",
    "pos_emb = model.pos_emb.cpu().detach().squeeze(0)\n",
    "fig, axes = plt.subplots(4, 4, figsize=(10,10))\n",
    "for i in range(16):\n",
    "    axes[i//4][i%4].imshow(pos_emb[i].reshape(4*7,2*7))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb8518f-f4d3-4203-8a1b-67525a271fa1",
   "metadata": {},
   "source": [
    "# Check Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef7b73a8-c57e-4af9-8dde-67e6840791a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "model = Net().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3fcf76b-b0a7-47fd-a163-dce3105cbd4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from tqdm.auto import tqdm\n",
    "from tensorboard_pytorch import TensorboardPyTorch\n",
    "\n",
    "def simple_trainer(model, loaders, criterion, optim, writer, epoch_start, epoch_end, phases=['train', 'test']):\n",
    "    for epoch in tqdm(range(epoch_start, epoch_end)):\n",
    "        for phase in phases:\n",
    "            running_acc = 0.0\n",
    "            running_loss = 0.0\n",
    "            model.train() if 'train' in phase else model.eval()\n",
    "            for x_true, y_true in loaders[phase]:\n",
    "                x_true, y_true = x_true.to(device), y_true.to(device)\n",
    "                y_pred = model(x_true)\n",
    "                loss = criterion(y_pred, y_true)\n",
    "                if phase == phases[0]:\n",
    "                    optim.zero_grad()\n",
    "                    loss.backward()\n",
    "                    optim.step()\n",
    "                running_acc += (torch.argmax(y_pred.detach().data, dim=1) == y_true).sum().item()\n",
    "                running_loss += loss.item() * x_true.size(0)\n",
    "\n",
    "            epoch_acc = running_acc / len(loaders[phase].dataset)\n",
    "            epoch_loss = running_loss / len(loaders[phase].dataset)\n",
    "            writer.log_scalar(f'Acc/{phase}', round(epoch_acc, 4), epoch + 1)\n",
    "            writer.log_scalar(f'Loss/{phase}', round(epoch_loss, 4), epoch + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c784e98c-4ce7-4cf0-b2b1-666387daa456",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "optim = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "date = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "writer = TensorboardPyTorch(f'tensorboard/check_trainer/{date}', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f979b54-5f75-4f23-85f7-ded61c668db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 100\n",
    "simple_trainer(model, loaders, criterion, optim, writer, epoch_start=0, epoch_end=EPOCHS, phases=['train', 'test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e4d035-fe9a-4a54-96f5-f4c78128353b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tldl",
   "language": "python",
   "name": "tldl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
