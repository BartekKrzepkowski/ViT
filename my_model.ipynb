{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d39ef611-b2b8-4a9b-9020-dcda4c2f00ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a1b429-0e3d-4ea6-a211-2e6dd60a1edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ed85b9e-ef8a-4931-b3dc-7a3b1eb8f274",
   "metadata": {},
   "source": [
    "# TODO\n",
    "\n",
    "> Zbudować model\n",
    "> podzielić zdjęcia na patche 4x4, albo 7x7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc227cfa-1e6d-487e-8a9d-e4da0d61116a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Net1(nn.Module):\n",
    "    def __init__(self, patch_dim, img_dims, n_layers, n_heads, mult_hidden=10):\n",
    "        super().__init__()\n",
    "        # sizes\n",
    "        hidden_dim = img_dims[0] * patch_dim ** 2\n",
    "        seq_len = np.product(img_dims) // hidden_dim\n",
    "        self.patch_dim = patch_dim\n",
    "        print(hidden_dim, seq_len, patch_dim)\n",
    "        \n",
    "        #input\n",
    "        self.init_proj = nn.Linear(hidden_dim, mult_hidden*hidden_dim)\n",
    "        self.init_layernorm = nn.LayerNorm(mult_hidden*hidden_dim)\n",
    "        self.init_dropout = nn.Dropout(0.1)\n",
    "        self.pos_emb = nn.Parameter(torch.zeros(1, seq_len, mult_hidden*hidden_dim))\n",
    "        \n",
    "        # transformer encoder + mlp\n",
    "        self.mhsa_layer = nn.TransformerEncoderLayer(mult_hidden*hidden_dim, n_heads, dim_feedforward=4*mult_hidden*hidden_dim, batch_first=True, activation=nn.GELU())\n",
    "        self.mhsa = nn.TransformerEncoder(self.mhsa_layer, num_layers=n_layers, norm=nn.LayerNorm(mult_hidden*hidden_dim))\n",
    "        self.mlp = nn.Sequential(nn.Linear(mult_hidden*hidden_dim, 2*mult_hidden*hidden_dim), nn.ReLU(), nn.Linear(2*mult_hidden*hidden_dim, 10))\n",
    "        \n",
    "        # additional parameters\n",
    "        self.att1 = nn.Parameter(torch.randn(seq_len, 1) / np.sqrt(seq_len))\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, mult_hidden*hidden_dim))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        x = self.init_proj(x)\n",
    "        img_patches = torch.cat(\n",
    "                (self.expand_cls_to_batch(batch_size), x), dim=1)\n",
    "        x = x + self.pos_emb\n",
    "        x = self.init_layernorm(x)\n",
    "        x = self.init_dropout(x)\n",
    "        x = self.mhsa(x)\n",
    "        x = self.mlp(x[:, 0, :])\n",
    "        return x\n",
    "    \n",
    "    def reduction_att(self, x):\n",
    "        # perform attention to reduce dimensinality\n",
    "        att = F.softmax(((x @ x.transpose(-2,-1)) @ self.att1 / x.size(-1)).transpose(-2,-1), dim=-1)\n",
    "        x = att @ x\n",
    "        x = x.flatten(-2)\n",
    "        return x\n",
    "    \n",
    "    def expand_cls_to_batch(self, batch):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            batch: batch size\n",
    "        Returns: cls token expanded to the batch size\n",
    "        \"\"\"\n",
    "        return self.cls_token.expand([batch, -1, -1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ed1e53-475f-416b-ae3a-8505d535510f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import MNIST, CIFAR10\n",
    "from torchvision.transforms import Compose, ToTensor, Normalize\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "transform = Compose([ToTensor(), Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "dataset_train = CIFAR10('./data', train=True, transform=transform, download=True)\n",
    "dataset_test = CIFAR10('./data', train=False, transform=transform, download=True)\n",
    "\n",
    "loader_train = DataLoader(dataset_train, batch_size=64, shuffle=True, pin_memory=True, num_workers=4)\n",
    "loader_test = DataLoader(dataset_test, batch_size=64, shuffle=False, pin_memory=True, num_workers=4)\n",
    "\n",
    "loaders = {\n",
    "    'train': loader_train,\n",
    "    'test': loader_test\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "770d800a-fb73-40df-a570-b6b326f55f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_patches_naive(x, step):\n",
    "    patches = []\n",
    "    for i in range(x.shape[-1]//step):\n",
    "        for j in range(x.shape[-1]//step):\n",
    "            patches.append(x[:,:,i*step:(i+1)*step,j*step:(j+1)*step].flatten(start_dim=1))\n",
    "    return torch.stack(patches, axis=0).transpose(0,1)\n",
    "\n",
    "# patches = get_patches_naive(x, 4)\n",
    "# patches.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29680358-01a9-422e-99b3-13a1e426aaf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from tqdm.auto import tqdm\n",
    "from tensorboard_pytorch import TensorboardPyTorch\n",
    "\n",
    "def simple_trainer(model, loaders, criterion, optim, writer, epoch_start, epoch_end, phases=['train', 'test']):\n",
    "    for epoch in tqdm(range(epoch_start, epoch_end)):\n",
    "        for phase in phases:\n",
    "            running_acc = 0.0\n",
    "            running_loss = 0.0\n",
    "            model.train() if 'train' in phase else model.eval()\n",
    "            for x_true, y_true in loaders[phase]:\n",
    "                x_true, y_true = x_true.to(device), y_true.to(device)\n",
    "                x_true = get_patches_naive(x_true, model.patch_dim)\n",
    "                y_pred = model(x_true)\n",
    "                loss = criterion(y_pred, y_true)\n",
    "                if phase == phases[0]:\n",
    "                    optim.zero_grad()\n",
    "                    loss.backward()\n",
    "                    optim.step()\n",
    "                running_acc += (torch.argmax(y_pred.detach().data, dim=1) == y_true).sum().item()\n",
    "                running_loss += loss.item() * x_true.size(0)\n",
    "\n",
    "            epoch_acc = running_acc / len(loaders[phase].dataset)\n",
    "            epoch_loss = running_loss / len(loaders[phase].dataset)\n",
    "            writer.log_scalar(f'Acc/{phase}', round(epoch_acc, 4), epoch + 1)\n",
    "            writer.log_scalar(f'Loss/{phase}', round(epoch_loss, 4), epoch + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b84a6c3f-6970-4142-bb40-dec20d2a4630",
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir=tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba659a5-7a95-4a38-bf69-2db42b25ea16",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 150\n",
    "import madgrad\n",
    "\n",
    "model = Net1(patch_dim=8, img_dims=(3, 32, 32), n_layers=8, n_heads=8, mult_hidden=4).to(device)\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "# optim = madgrad.MADGRAD(model.parameters(), lr=1e-2, momentum=0.9)\n",
    "optim = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=150)\n",
    "\n",
    "date = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "writer = TensorboardPyTorch(f'tensorboard/ViT/cifar10/sgd/cls_token/pos_emb_8x8x8x4_extended/{date}', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6edd2f53-a5f1-4fa1-b564-aa995006d47f",
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_trainer(model, loaders, criterion, optim, writer, epoch_start=0, epoch_end=EPOCHS, phases=['train', 'test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff31c9b-bbf6-41b4-bb9e-5711b647530f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a15380b5-976d-408f-b2b8-db117e92d5c3",
   "metadata": {},
   "source": [
    "# Setting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "140092d4-fe34-4b79-a45a-241dded72fa1",
   "metadata": {},
   "source": [
    "# Learned Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dbd1ab3-8b9f-4819-8e70-51caee5ae27e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4839188-8a7e-4a37-8f12-e79b65c717a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(model.pos_emb.detach().cpu().squeeze(0))\n",
    "plt.xlabel('Emb Dim')\n",
    "plt.ylabel('Position')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b5ce9fc-3fea-4dd2-a538-3227bc201421",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.pos_emb.detach().cpu().squeeze(0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee94279c-ce7c-4b22-9155-9ca075e599c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a41a6e69-23cc-449e-866c-71138e57fa29",
   "metadata": {},
   "outputs": [],
   "source": [
    "768 // 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ccef54-2458-404c-a5a8-9088a7286883",
   "metadata": {},
   "outputs": [],
   "source": [
    "mult = 4\n",
    "\n",
    "pos_emb = model.pos_emb.cpu().detach().squeeze(0)\n",
    "fig, axes = plt.subplots(4, 4, figsize=(10,10))\n",
    "for i in range(16):\n",
    "    axes[i//4][i%4].imshow(pos_emb[i].reshape(8*4,8*3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb8518f-f4d3-4203-8a1b-67525a271fa1",
   "metadata": {},
   "source": [
    "# Check Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef7b73a8-c57e-4af9-8dde-67e6840791a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "net = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3fcf76b-b0a7-47fd-a163-dce3105cbd4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from tqdm.auto import tqdm\n",
    "from tensorboard_pytorch import TensorboardPyTorch\n",
    "\n",
    "def simple_trainer(model, loaders, criterion, optim, writer, epoch_start, epoch_end, phases=['train', 'test']):\n",
    "    for epoch in tqdm(range(epoch_start, epoch_end)):\n",
    "        for phase in phases:\n",
    "            running_acc = 0.0\n",
    "            running_loss = 0.0\n",
    "            model.train() if 'train' in phase else model.eval()\n",
    "            for x_true, y_true in loaders[phase]:\n",
    "                x_true, y_true = x_true.to(device), y_true.to(device)\n",
    "                y_pred = model(x_true)\n",
    "                loss = criterion(y_pred, y_true)\n",
    "                if phase == phases[0]:\n",
    "                    optim.zero_grad()\n",
    "                    loss.backward()\n",
    "                    optim.step()\n",
    "                running_acc += (torch.argmax(y_pred.detach().data, dim=1) == y_true).sum().item()\n",
    "                running_loss += loss.item() * x_true.size(0)\n",
    "\n",
    "            epoch_acc = running_acc / len(loaders[phase].dataset)\n",
    "            epoch_loss = running_loss / len(loaders[phase].dataset)\n",
    "            writer.log_scalar(f'Acc/{phase}', round(epoch_acc, 4), epoch + 1)\n",
    "            writer.log_scalar(f'Loss/{phase}', round(epoch_loss, 4), epoch + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c784e98c-4ce7-4cf0-b2b1-666387daa456",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "optim = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "date = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "writer = TensorboardPyTorch(f'tensorboard/check_trainer/{date}', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e193ff-bcb8-4b8c-9997-1d2c267cba54",
   "metadata": {},
   "source": [
    "# Pytorch lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb957a45-29ba-49ea-802d-ec8863f06a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyLightningModule()\n",
    "\n",
    "trainer = Trainer()\n",
    "trainer.fit(model, train_dataloader, val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8de84ef-f3dc-44ba-8851-5054ab5cd1eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision import transforms\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class Net1(pl.LightningModule):\n",
    "    def __init__(self, patch_dim, img_dims, n_layers, n_heads, mult_hidden=10):\n",
    "        super().__init__()\n",
    "        # sizes\n",
    "        hidden_dim = img_dims[0] * patch_dim ** 2\n",
    "        seq_len = np.product(img_dims) // hidden_dim\n",
    "        self.patch_dim = patch_dim\n",
    "        print(hidden_dim, seq_len, patch_dim)\n",
    "        \n",
    "        #input\n",
    "        self.init_proj = nn.Linear(hidden_dim, mult_hidden*hidden_dim)\n",
    "        self.init_layernorm = nn.LayerNorm(mult_hidden*hidden_dim)\n",
    "        self.init_dropout = nn.Dropout(0.1)\n",
    "        self.pos_emb = nn.Parameter(torch.zeros(1, seq_len, mult_hidden*hidden_dim))\n",
    "        \n",
    "        # transformer encoder + mlp\n",
    "        self.mhsa_layer = nn.TransformerEncoderLayer(mult_hidden*hidden_dim, n_heads, dim_feedforward=4*mult_hidden*hidden_dim, batch_first=True, activation=nn.GELU())\n",
    "        self.mhsa = nn.TransformerEncoder(self.mhsa_layer, num_layers=n_layers, norm=nn.LayerNorm(mult_hidden*hidden_dim))\n",
    "        self.mlp = nn.Sequential(nn.Linear(mult_hidden*hidden_dim, 2*mult_hidden*hidden_dim), nn.ReLU(), nn.Linear(2*mult_hidden*hidden_dim, 10))\n",
    "        \n",
    "        # additional parameters\n",
    "        self.att1 = nn.Parameter(torch.randn(seq_len, 1) / np.sqrt(seq_len))\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, mult_hidden*hidden_dim))\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        x = self.init_proj(x)\n",
    "        img_patches = torch.cat(\n",
    "                (self.expand_cls_to_batch(batch_size), x), dim=1)\n",
    "        x = x + self.pos_emb\n",
    "        x = self.init_layernorm(x)\n",
    "        x = self.init_dropout(x)\n",
    "        x = self.mhsa(x)\n",
    "        x = self.mlp(x[:, 0, :])\n",
    "        return x\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "        return optimizer\n",
    "\n",
    "    def training_step(self, train_batch, batch_idx):\n",
    "        x, y = train_batch\n",
    "        x = get_patches_naive(x, 7)\n",
    "        z = self.forward(x)    \n",
    "        loss = self.criterion(z, y)\n",
    "        self.log('train_loss', loss)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, val_batch, batch_idx):\n",
    "        x, y = val_batch\n",
    "        x = get_patches_naive(x, 7)\n",
    "        z = self.forward(x)\n",
    "        loss = self.criterion(z, y)\n",
    "        self.log('val_loss', loss)\n",
    "    \n",
    "    def reduction_att(self, x):\n",
    "        # perform attention to reduce dimensinality\n",
    "        att = F.softmax(((x @ x.transpose(-2,-1)) @ self.att1 / x.size(-1)).transpose(-2,-1), dim=-1)\n",
    "        x = att @ x\n",
    "        x = x.flatten(-2)\n",
    "        return x\n",
    "    \n",
    "    def expand_cls_to_batch(self, batch):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            batch: batch size\n",
    "        Returns: cls token expanded to the batch size\n",
    "        \"\"\"\n",
    "        return self.cls_token.expand([batch, -1, -1])\n",
    "    \n",
    "    def get_patches_naive(self, x, step):\n",
    "        patches = []\n",
    "        for i in range(x.shape[-1]//step):\n",
    "            for j in range(x.shape[-1]//step):\n",
    "                patches.append(x[:,:,i*step:(i+1)*step,j*step:(j+1)*step].flatten(start_dim=1))\n",
    "        return torch.stack(patches, axis=0).transpose(0,1)\n",
    "\n",
    "# patches = get_patches_naive(x, 4)\n",
    "# patches.shape\n",
    "\n",
    "\n",
    "\n",
    "# data\n",
    "dataset = MNIST('', train=True, download=True, transform=transforms.ToTensor())\n",
    "mnist_train, mnist_val = random_split(dataset, [55000, 5000])\n",
    "\n",
    "train_loader = DataLoader(mnist_train, batch_size=64)\n",
    "val_loader = DataLoader(mnist_val, batch_size=64)\n",
    "\n",
    "# model\n",
    "model = Net1(patch_dim=7, img_dims=(1, 28, 28), n_layers=7, n_heads=7, mult_hidden=4).to(device)\n",
    "\n",
    "# training\n",
    "trainer = pl.Trainer(gpus=1, num_nodes=1, precision=16, limit_train_batches=0.5)\n",
    "trainer.fit(model, train_loader, val_loader)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d3113e-8ebd-46c2-8cbb-5f8a5f611d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext tensorboard\n",
    "%tensorboard --logdir=lightning_logs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f54802-6e2b-4919-8f38-e56380cf5c14",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tldl",
   "language": "python",
   "name": "tldl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
